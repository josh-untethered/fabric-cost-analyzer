{"cells":[{"cell_type":"markdown","source":["#### Fabric POC Cost Analyzer v0.2\n","###### This Cost Analyzer notebook will allow you to track the true cost per query of a Fabric Warehouse or Lakehouse Query based on the amount of Capacity Units (CUs) consumed and will help you estimate the capacity size you will need to run these queries. \n","###### Due to the ability of Fabric Warehouse and Lakhouse SQL Endpoints to [burst up to 12X the workspace capacity](https://learn.microsoft.com/en-us/fabric/data-warehouse/burstable-capacity#sku-guardrails) it is important to understand this behavior on a per query basis to choose the appropriate [Fabric Capacity F SKU](https://azure.microsoft.com/en-us/pricing/details/microsoft-fabric/) that you can use to calculate your Fabric TCO. \n","\n","###### This notebook accomplishes this by using [Semantic Link](https://learn.microsoft.com/en-us/fabric/data-science/semantic-link-overview) to query data from the [Capacity Metrics App](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app) and correlates it to query information from [Query Insights](https://learn.microsoft.com/en-us/fabric/data-warehouse/query-insights). However, due to Capacity Metrics information only being available via DAX queries against the semantic model, this approach may take up to 3-4+ hours to run. Logging has been added to estimate how long the notebook will take to run."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a6603fe6-51f2-4160-b331-e341fcddd0ed"},{"cell_type":"markdown","source":["###### **Pre-Requisites**\n","###### 1. [Install the Capacity Metrics App](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app-install?tabs=1st).\n","###### 2. Capacity Admin should get the capacity ID used for this workspace from the Admin Portal.\n","###### 3. Create and attach a Fabric Spark environment that includes the `semantic-link` pypi library.\n","###### 4. Create and attach a Lakehouse to this notebook.\n","###### 5. All Warehouse and Lakehouse Query runs need to be done on Lakehouses or Warehouses in this workspace. If not, you need to create shortcuts to those tables in other workspaces within the Lakehouse attached to this notebook.\n","###### 6. Replace the parameters in cell 4 with the appropriate values."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cab4302b-8cdc-431f-853c-99838db988d5"},{"cell_type":"code","source":["# Import Libraries\n","from datetime import datetime, timedelta\n","from sempy.fabric import evaluate_dax\n","import warnings\n","from pyspark.sql.utils import AnalysisException\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n","from tqdm import tqdm\n","import time\n","import pandas as pd\n","from sempy import fabric\n","\n","# Suppress specific UserWarnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Ambiguous column name\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9cb244e4-9aa1-4645-9268-57308a33ecac"},{"cell_type":"code","source":["# Capacity Metrics App Semantic Model Name - 'Fabric Capacity Metrics' is the default name\n","dataset = '<>' \n","\n","# Capacity Id - capacity admin can find this in Admin Portal\n","capacity_id = '<>' \n","\n","# Get workspace name\n","workspace_name = fabric.resolve_workspace_name(fabric.get_workspace_id())\n","\n","# Integer or Decimal number of days to look at. Can set to \"all\" to include all usage data\n","number_of_days = \"<>\"\n","\n","# Optional parameter to reduce the number of timepoints searched for testing. Set to \"no\" to include all timepoints. Start with a smaller number to test.\n","limit_num_timepoints = \"<>\"\n","\n","# Optional parameter to filter to a list of Fabric data items to collect usage from. Set to \"no\" to include all data items.\n","limit_data_items = ['<>']\n","\n","# Adjust based on the CU regional pricing for your capacity\n","regional_pricing_per_cu = #\n","\n","# Name of the lakehouse you created and attached to this workspace\n","cost_analyzer_lakehouse = '<>'"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"8116bd75-aa39-41e3-a191-917567546bc4"},{"cell_type":"code","source":["# Define the schema for the DataFrame\n","dataframe_schema = StructType([\n","    StructField(\"OperationID\", StringType(), True),\n","    StructField(\"Status\", StringType(), True),\n","    StructField(\"User\", StringType(), True),\n","    StructField(\"Operation\", StringType(), True),\n","    StructField(\"WorkspaceName\", StringType(), True),\n","    StructField(\"Item\", StringType(), True),\n","    StructField(\"ItemName\", StringType(), True),\n","    StructField(\"DurationSec\", DoubleType(), True),\n","    StructField(\"TotalCUSec\", DoubleType(), True),\n","    StructField(\"CapacityCU\", DoubleType(), True),\n","    StructField(\"capacityId\", StringType(), True),\n","    StructField(\"OperationType\", StringType(), True)\n","])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8e9ee161-c566-4ce8-aa0a-34f548150ef0"},{"cell_type":"code","source":["# Refresh semantic model to ensure latest data is available\n","# fabric.refresh_dataset(dataset = dataset, refresh_type = \"full\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08095967-e30d-472d-a6be-a400f74d58f2"},{"cell_type":"markdown","source":["###### Check if the semantic model was refreshed. Continue after it is refreshed."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a3d5ee61-92ff-49ac-9315-d1f83ee56e65"},{"cell_type":"code","source":["def format_time(seconds):\n","    \"\"\"\n","    Function to format the time in a human-readable format.\n","    \"\"\"\n","    if seconds < 60:\n","        return f\"{seconds:.2f} seconds\"\n","    elif seconds < 3600:\n","        minutes = seconds / 60\n","        return f\"{minutes:.2f} minutes\"\n","    else:\n","        hours = seconds / 3600\n","        return f\"{hours:.2f} hours\"\n","\n","def get_dax_query_filtered_timepoints():\n","    dax_query_filtered_timepoints = \"\"\"\n","    EVALUATE\n","    FILTER (\n","        VALUES('TimePoints'[TimePoint]),\n","        CALCULATE([Cumulative CU Usage (s)]) > 0\n","    )\n","    \"\"\"\n","    return dax_query_filtered_timepoints\n","\n","def insert_or_append(df, table_name):\n","    \"\"\"\n","    Function to insert or append the DataFrame into the specified Delta table.\n","    \"\"\"\n","    df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n","\n","def fetch_and_process_dax(timepoint, capacity_id, limit_data_items):\n","    timepoint_str = timepoint.strftime(\"%Y-%m-%d %H:%M:%S\")\n","    timepoint_dt = datetime.strptime(timepoint_str, \"%Y-%m-%d %H:%M:%S\")\n","    current_year = str(timepoint_dt.year)\n","    current_month = str(timepoint_dt.month)\n","    current_day = str(timepoint_dt.day)\n","    starting_hour = str(timepoint_dt.hour)\n","    starting_minutes = str(timepoint_dt.minute)\n","    starting_seconds = str(timepoint_dt.second)\n","\n","    dax_background_operation = f'''\n","        DEFINE\n","            MPARAMETER 'CapacityID' = \"{capacity_id}\"\n","            MPARAMETER 'TimePoint' = (DATE({current_year}, {current_month}, {current_day}) + TIME({starting_hour}, {starting_minutes}, {starting_seconds}))\n","            VAR varFilter_Capacity = TREATAS({{\"{capacity_id}\"}}, 'Capacities'[capacityId])\n","            VAR varFilter_TimePoint = \n","                TREATAS(\n","                    {{(DATE({current_year}, {current_month}, {current_day}) + TIME({starting_hour}, {starting_minutes}, {starting_seconds}))}},\n","                    'TimePoints'[TimePoint]\n","                )\n","            VAR varTable_Details =\n","                SUMMARIZECOLUMNS(\n","                    'TimePointBackgroundDetail'[OperationId],\n","                    'TimePointBackgroundDetail'[Status],\n","                    'TimePointBackgroundDetail'[User],\n","                    'TimePointBackgroundDetail'[Operation],\n","                    'Items'[WorkspaceName],\n","                    'Items'[ItemKind],\n","                    'Items'[ItemName],\n","                    'TimePointBackgroundDetail'[Capacity CU (s)],\n","                    varFilter_Capacity,\n","                    varFilter_TimePoint,\n","                    \"DurationSec\", SUM('TimePointBackgroundDetail'[Duration (s)]),\n","                    \"TotalCUSec\", CALCULATE(SUM('TimePointBackgroundDetail'[Total CU (s)]))\n","                )\n","        EVALUATE  SELECTCOLUMNS(\n","            varTable_Details,\n","            \"OperationID\", [OperationId],\n","            \"Status\", [Status],\n","            \"User\", [User],\n","            \"Operation\", [Operation],\n","            \"WorkspaceName\", [WorkspaceName],\n","            \"Item\", [ItemKind],\n","            \"ItemName\", [ItemName],\n","            \"DurationSec\", [DurationSec],\n","            \"TotalCUSec\", [TotalCUSec],\n","            \"CapacityCU\", [Capacity CU (s)] / 30\n","        )'''\n","\n","    # Evaluate the DAX query and get the result\n","    df_dax_result = evaluate_dax(dataset, dax_background_operation)\n","\n","    # Rename columns to remove brackets\n","    df_dax_result.columns = [col.strip('[]') for col in df_dax_result.columns]\n","\n","    # Add capacityId and OperationType columns, and remove duplicates based on OperationID\n","    if not df_dax_result.empty:\n","        df_dax_result['capacityId'] = capacity_id\n","        df_dax_result['OperationType'] = 'background'\n","        df_dax_result = df_dax_result.drop_duplicates(subset=['OperationID'])\n","        return df_dax_result\n","    else:\n","        return pd.DataFrame()\n","\n","def generate_dax_background_operation(filtered_timepoints, capacity_id, limit_num_timepoints=\"no\", limit_data_items=\"no\"):\n","    if not filtered_timepoints:\n","        print(\"No timepoints to process.\")\n","        return\n","\n","    if limit_num_timepoints != \"no\":\n","        limit_num_timepoints = int(limit_num_timepoints)\n","        filtered_timepoints = filtered_timepoints[:limit_num_timepoints]\n","\n","    if not filtered_timepoints:\n","        print(\"No timepoints to process after applying the limit.\")\n","        return\n","\n","    total_timepoints = len(filtered_timepoints)\n","    start_time = time.time()\n","    \n","    results = []\n","    estimated_time_per_timepoint = 0\n","    progress_bar = tqdm(filtered_timepoints, desc=\"Processing Timepoints\", unit=\"timepoint\", mininterval=1, dynamic_ncols=True)\n","\n","    for i, timepoint in enumerate(progress_bar):\n","        df_dax_result = fetch_and_process_dax(timepoint, capacity_id, limit_data_items)\n","        \n","        if not df_dax_result.empty:\n","            results.append(df_dax_result)\n","        else:\n","            print(f\"No data extracted for timepoint {timepoint}\")\n","        \n","        elapsed_time = time.time() - start_time\n","        estimated_time_per_timepoint = elapsed_time / (i + 1)\n","        remaining_time = estimated_time_per_timepoint * (total_timepoints - (i + 1))\n","        remaining_time_formatted = format_time(remaining_time)\n","        \n","        progress_bar.set_postfix_str(f\"Est. remaining time: {remaining_time_formatted}\")\n","\n","    if results:\n","        combined_df = pd.concat(results, ignore_index=True)\n","        \n","        numeric_columns = [\n","            \"DurationSec\", \"TotalCUSec\", \"CapacityCU\"\n","        ]\n","\n","        for col in numeric_columns:\n","            combined_df[col] = combined_df[col].astype(float)\n","        \n","        combined_df = combined_df.drop_duplicates(subset=['OperationID'])\n","\n","        spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n","        combined_df_spark = spark.createDataFrame(combined_df, schema=dataframe_schema)\n","        spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n","\n","        insert_or_append(combined_df_spark, \"bronze_all_background_operations\")\n","        print(f\"Inserted {len(combined_df)} rows into bronze_all_background_operations\")\n","    else:\n","        print(\"No data to insert into bronze_all_background_operations\")\n","\n","    elapsed_time = time.time() - start_time\n","    elapsed_time_formatted = format_time(elapsed_time)\n","    print(f'Progress: Complete, Total time: {elapsed_time_formatted}')\n","\n","def filter_and_save_operations(limit_data_items, workspace_name):\n","    df_bronze = spark.table(\"bronze_all_background_operations\")\n","\n","    if limit_data_items != \"no\":\n","        df_bronze = df_bronze.filter(df_bronze[\"ItemName\"].isin(limit_data_items))\n","    df_bronze = df_bronze.filter(df_bronze[\"Operation\"].isin([\"Warehouse Query\", \"SQL Endpoint Query\"]))\n","    df_bronze = df_bronze.filter(df_bronze[\"WorkspaceName\"] == workspace_name)\n","    df_bronze = df_bronze.filter(df_bronze[\"User\"] != \"System\")\n","\n","    insert_or_append(df_bronze, \"silver_dw_lh_queries\")\n","    print(f\"Inserted {df_bronze.count()} rows into silver_dw_lh_queries\")\n","\n","def ensure_table_exists(table_name, schema):\n","    \"\"\"\n","    Function to ensure that a table exists. If it doesn't, create an empty table with the given schema.\n","    \"\"\"\n","    try:\n","        spark.table(table_name)\n","    except AnalysisException:\n","        print(f\"Table '{table_name}' does not exist, creating now...\")\n","        empty_df = spark.createDataFrame([], schema)\n","        empty_df.write.format(\"delta\").saveAsTable(table_name)\n","\n","def ensure_tables_exist():\n","    \"\"\"\n","    Function to ensure that the necessary tables exist.\n","    \"\"\"\n","    ensure_table_exists(\"bronze_all_background_operations\", dataframe_schema)\n","    ensure_table_exists(\"silver_dw_lh_queries\", dataframe_schema)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"73ecff1b-ce06-4ea4-8015-7ecbf57c3959"},{"cell_type":"code","source":["# Ensure the necessary tables exist\n","ensure_tables_exist()\n","\n","# Get the DAX query for filtered timepoints\n","dax_query_filtered_timepoints = get_dax_query_filtered_timepoints()\n","\n","# Evaluate the DAX query and get the filtered timepoints\n","df_filtered_timepoints = evaluate_dax(dataset, dax_query_filtered_timepoints)\n","\n","# Extract the filtered timepoints from the query result\n","if 'TimePoints[TimePoint]' in df_filtered_timepoints.columns:\n","    filtered_timepoints = df_filtered_timepoints['TimePoints[TimePoint]'].tolist()\n","else:\n","    print(\"Column 'TimePoints[TimePoint]' not found. Please check the DataFrame.\")\n","    filtered_timepoints = df_filtered_timepoints.iloc[:, 0].tolist()\n","\n","# Calculate the cutoff date if number_of_days is not \"all\"\n","if number_of_days != \"all\":\n","    cutoff_date = datetime.now() - timedelta(days=number_of_days)\n","    filtered_timepoints = [tp for tp in filtered_timepoints if tp >= cutoff_date]\n","else:\n","    cutoff_date = None\n","\n","print(f\"Cutoff date: {cutoff_date}\")\n","\n","# Run the function with the updated parameters\n","generate_dax_background_operation(filtered_timepoints, capacity_id, limit_num_timepoints, limit_data_items)\n","\n","# Filter and save operations\n","filter_and_save_operations(limit_data_items, workspace_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"03f28d15-a9ef-48ac-a0e3-6b4df2c34daf"},{"cell_type":"markdown","source":["###### Check the top rows of the bronze and silver tables to validate that records were written"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"57719e4a-37bd-4e2a-a8e4-b6583c5cc901"},{"cell_type":"code","source":["display(spark.sql(f\"SELECT * FROM {cost_analyzer_lakehouse}.bronze_all_background_operations LIMIT 5\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d6e0b5b5-3c7c-42e5-b6e3-6e32024963db"},{"cell_type":"code","source":["display(spark.sql(f\"SELECT * FROM {cost_analyzer_lakehouse}.silver_dw_lh_queries LIMIT 5\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"240fec5d-f929-4769-9246-83246a90b453"},{"cell_type":"code","source":["# Read the background_operations table\n","queries_df = spark.sql(f\"SELECT DISTINCT WorkspaceName, Item, ItemName FROM {cost_analyzer_lakehouse}.silver_dw_lh_queries\")\n","\n","# Collect unique values from the DataFrame\n","unique_items = queries_df.collect()\n","\n","# Construct the T-SQL query to union all query insights and join with background operations\n","query_parts = []\n","for row in unique_items:\n","    item_name = row['ItemName']\n","    \n","    # Calculate the recommended capacity in CU\n","    rec_capacity_cu = f\"\"\"\n","    CASE\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 2 THEN 'F2'\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 4 THEN 'F4'\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 8 THEN 'F8'\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 16 THEN 'F16'\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 32 THEN 'F32'\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 64 THEN 'F64'\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 128 THEN 'F128'\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 256 THEN 'F256'\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 512 THEN 'F512'\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 1024 THEN 'F1024'\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 2048 THEN 'F2048'\n","        ELSE 'Above F2048'\n","    END\"\"\"\n","\n","    # Construct the query part for each unique item\n","    query_part = f\"\"\"\n","SELECT\n","    '{item_name}' AS DataItem,\n","    a.distributed_statement_id,\n","    a.login_name,\n","    a.command,\n","    a.start_time,\n","    a.end_time,\n","    ROUND(b.DurationSec, 2) AS DurationSec,\n","    ROUND(b.DurationSec / 3600, 5) AS DurationHr,\n","    ROUND(b.TotalCUSec, 2) AS TotalCUs,\n","    b.CapacityCU AS ExpectedCUsPerSec,\n","    ROUND(b.TotalCUSec / NULLIF(b.DurationSec, 0), 2) AS ActualCUsPerSec,\n","    CASE\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) > b.CapacityCU THEN 'Yes'\n","        ELSE 'No'\n","    END AS IsBursting,\n","    ROUND(\n","        CASE\n","            WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) > b.CapacityCU THEN b.TotalCUSec / NULLIF(b.DurationSec, 0) / b.CapacityCU\n","            ELSE b.TotalCUSec / NULLIF(b.DurationSec, 0) / b.CapacityCU\n","        END, 2) AS BurstingMultiplier,\n","    {regional_pricing_per_cu} AS RegionalCUPerHourCost,\n","    {regional_pricing_per_cu} * ROUND(b.DurationSec / 3600, 5) * b.CapacityCU AS ExpectedCostQuery,\n","    {regional_pricing_per_cu} * ROUND(b.DurationSec / 3600, 5) * ROUND(b.TotalCUSec / NULLIF(b.DurationSec, 0), 2) AS ActualCostQuery,\n","    CASE\n","        WHEN b.CapacityCU <= 2 THEN 'F2'\n","        WHEN b.CapacityCU <= 4 THEN 'F4'\n","        WHEN b.CapacityCU <= 8 THEN 'F8'\n","        WHEN b.CapacityCU <= 16 THEN 'F16'\n","        WHEN b.CapacityCU <= 32 THEN 'F32'\n","        WHEN b.CapacityCU <= 64 THEN 'F64'\n","        WHEN b.CapacityCU <= 128 THEN 'F128'\n","        WHEN b.CapacityCU <= 256 THEN 'F256'\n","        WHEN b.CapacityCU <= 512 THEN 'F512'\n","        WHEN b.CapacityCU <= 1024 THEN 'F1024'\n","        WHEN b.CapacityCU <= 2048 THEN 'F2048'\n","        ELSE 'Above F2048'\n","    END AS CapacityUsed,\n","    {rec_capacity_cu} AS CapacityRec,\n","    CAST(ROUND(b.CapacityCU * {regional_pricing_per_cu} * 24 * 30, 0) AS INT) AS CapacityUsedMonthlyCost,\n","    CAST(ROUND(b.CapacityCU * {regional_pricing_per_cu} * 24 * 365, 0) AS INT) AS CapacityUsedYearlyCost,\n","    CAST(ROUND((CASE\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 2 THEN 2\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 4 THEN 4\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 8 THEN 8\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 16 THEN 16\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 32 THEN 32\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 64 THEN 64\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 128 THEN 128\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 256 THEN 256\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 512 THEN 512\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 1024 THEN 1024\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 2048 THEN 2048\n","        ELSE 0\n","    END) * {regional_pricing_per_cu} * 24 * 30, 0) AS INT) AS CapacityRecMonthlyCost,\n","    CAST(ROUND((CASE\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 2 THEN 2\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 4 THEN 4\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 8 THEN 8\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 16 THEN 16\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 32 THEN 32\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 64 THEN 64\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 128 THEN 128\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 256 THEN 256\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 512 THEN 512\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 1024 THEN 1024\n","        WHEN b.TotalCUSec / NULLIF(b.DurationSec, 0) <= 2048 THEN 2048\n","        ELSE 0\n","    END) * {regional_pricing_per_cu} * 24 * 365, 0) AS INT) AS CapacityRecYearlyCost\n","FROM [{item_name}].[queryinsights].[exec_requests_history] a\n","INNER JOIN [{cost_analyzer_lakehouse}].[dbo].[silver_dw_lh_queries] b\n","ON a.distributed_statement_id = b.OperationID\n","WHERE a.distributed_statement_id IS NOT NULL AND b.DurationSec > 0 AND a.status = 'Succeeded'\"\"\"\n","    \n","    query_parts.append(query_part)\n","\n","# Combine all query parts into one T-SQL query with UNION ALL\n","final_query_with_join = \" UNION ALL \".join(query_parts)\n","\n","# Wrap the final query with the WITH clause and SELECT statement, sorted by start_time\n","final_tsql_query = f\"\"\"\n","WITH Metrics AS (\n","{final_query_with_join}\n",")\n","SELECT \n","    DataItem,\n","    distributed_statement_id,\n","    login_name,\n","    command,\n","    start_time,\n","    end_time,\n","    DurationSec,\n","    ROUND(DurationSec / 3600, 5) AS DurationHr,\n","    TotalCUs,\n","    ExpectedCUsPerSec,\n","    ActualCUsPerSec,\n","    IsBursting,\n","    BurstingMultiplier,\n","    RegionalCUPerHourCost,\n","    {regional_pricing_per_cu} * ROUND(DurationSec / 3600, 5) * ExpectedCUsPerSec AS ExpectedCostQuery,\n","    {regional_pricing_per_cu} * ROUND(DurationSec / 3600, 5) * ActualCUsPerSec AS ActualCostQuery,\n","    CapacityUsed,\n","    CapacityRec,\n","    CapacityUsedMonthlyCost,\n","    CapacityUsedYearlyCost,\n","    CapacityRecMonthlyCost,\n","    CapacityRecYearlyCost\n","FROM Metrics\n","ORDER BY start_time;\n","\"\"\"\n","\n","# Print the final T-SQL query with join\n","print(final_tsql_query)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"540f4881-53a7-43d4-80e2-6b0a3c6cd2b2"},{"cell_type":"markdown","source":["###### Copy the T-SQL code above and run it via the Lakehouse SQL Endpoint of the Lakehouse attached to this notebook. This will allow you to join the information from the Capacity Metrics App to the Query Insights DMV."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5add965e-2e6c-4000-b454-53d2160ad09d"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"ba8773bc-035d-4ccd-b38c-12ee6f288380","default_lakehouse_name":"taxi_lh","default_lakehouse_workspace_id":"3a9a87ab-469a-49ee-83b9-4348b46e2266"},"environment":{"environmentId":"3adb781d-7f3c-4140-acc0-99ac79b7db3b","workspaceId":"3a9a87ab-469a-49ee-83b9-4348b46e2266"}}},"nbformat":4,"nbformat_minor":5}